"白发三千丈 缘愁似个长 不知明镜里何处得秋霜",
"白日依山尽,黄河入海流欲穷千里目 更上一层楼",
"君不见黄河之水天上来 奔流到海不复回 君不见高堂明镜悲白发 朝如青丝暮成雪",
"天门中断楚江开 碧水东流至此回
两岸青山相对出 孤帆一片日边来","北国风光 千里冰封 万里雪飘")
tests=unlist(sapply(z,segmentCN))
test=lapply(X=tests,FUN=strsplit," ")
testTable=table(unlist(test))
testTable[1]
testTable[[1]]
testTable[2]
testTable=rev(sort(testTable))
names(testTable)
df=data.dframe(词汇=names(testTable),词频=testTable)
df=data.frame(词汇=names(testTable),词频=testTable)
df
df=data.frame(testTable)
df
wordcloud(df,random.color = TRUE,random.order = TRUE)
wordcloud(df$Var1,df$freq,random.color = TRUE,random.order = TRUE)
wordcloud(df$Var1,df$Freq,random.color = TRUE,random.order = TRUE)
wordcloud(df$Var1,df$Freq)
df
wordcloud(df$Var1,df$Freq)
wordcloud2(df$Var1,df$Freq)
library(wordcloud2)
wordcloud2(df$Var1,df$Freq)
wordcloud2(df)
hTable=data.frame(hTable)
head(hTable)
head(hTable,10)
head(hTable,100)
hTable=subset(hTablem,nchar(as.character(hTable$Var1))>1& hTable$Freq>100)
hTable=subset(hTable,nchar(as.character(hTable$Var1))>1& hTable$Freq>100)
wordcloud2(hTable)
wordcloud2(hTable)
hTable
hTable=subset(hTable,nchar(as.character(hTable$Var1))>1& hTable$Freq>50)
hTable
wordcloud2(hTable)
hTable=subset(hTable,nchar(as.character(hTable$Var1))>1& hTable$Freq>20)
hTable
wordcloud2(hTable)
hTable
hTable=table(unlist(word))
hTable=data.frame(hTable)
hTable
wordcloud2(hTable)
hTable=subset(hTable,nchar(as.character(hTable$Var1))>1)
hTable
wordcloud2(hTable)
wordcloud2(df)
honglou=file.choose()
lecture=read.csv(honglou,stringsAsFactors = FALSE)
lecture=read.csv(honglou,stringsAsFactors = FALSE)
res=lecture[]
honglou1=file.choose()
lecture1=read.csv(honglou1,stringsAsFactors = FALSE)
lecture1=read.csv(honglou1,stringsAsFactors = FALSE,header=FALSE)
?read.csv
lecture1=read.csv(honglou1, fileEncoding="UTF-8",stringsAsFactors = FALSE,header=FALSE)
lecture1=read.csv(honglou1, fileEncoding="UTF-8",stringsAsFactors = FALSE,header=FALSE)
res1=lecture1[]
words=unlist(lapply(X=res1,FUN=segmentCN)
word=lapply(X=words,FUN=strsplit,' ')
word=lapply(X=words,FUN=strsplit,' ')
word=lapply(X=words,FUN=strsplit,' ')
word
head(word)
hTable=table(unlist(word))
hTable=rev(sort(hTable))
hTable=rev(sort(hTable))
hdf=data.frame(hTable)
hdf
wordcloud2(hdf)
lecture1=read.csv(honglou1, stringsAsFactors = FALSE,header=FALSE)
res1=lecture1[]
words=unlist(lapply(X=res1,FUN=segmentCN))
word=lapply(X=words,FUN=strsplit,' ')
head(word)
hTable=table(unlist(word))
hTable=rev(sort(hTable))
hdf=data.frame(hTable)
wordcloud2(hdf)
hdf=subset(hdf,nchar(as.character(hdf$Var1)>1)
wordcloud2(hdf)
hdf=subset(hdf,nchar(as.character(hdf$Var1)>1))
hdf=subset(hdf,nchar(as.character(hdf$Var1))>1)
wordcloud2(hdf)
hdf
hdf=data.frame(hTable)
head(hdf)
wordcloud2(head(hdf))
lecture1=read.csv(honglou1,Encoding="UTF-8" stringsAsFactors = FALSE,header=FALSE)
lecture1=read.csv(honglou1,fileEncoding="UTF-8",stringsAsFactors = FALSE,header=FALSE)
res1=lecture1[]
words=unlist(lapply(X=res1,FUN=segmentCN))
word=lapply(X=words,FUN=strsplit,' ')
head(word)
hTable=table(unlist(word))
hTable=rev(sort(hTable))
hdf=data.frame(hTable)
head(hdf)
hdf=subset(hdf,nchar(as.character(hdf$Var1))>1)
head(hdf)
hdf=subset(hdf,nchar(as.character(hdf$Var1))>1& hdf$Freq>100)
head(hdf)
wordcloud2(head(hdf))
hdf
wordcloud2(hdf)
class(words)
words
View(words)
wordcloud2(hdf,shape = squre)
wordcloud2(hdf,shape = tringle)
wordcloud2(hdf,shape = "tringle")
?wordcloud2
wordcloud2(hdf,shape = "cloud")
wordcloud2(hdf,shape = "diamond")
class(word)
hTable=table(unlist(word))
#hTable=rev(sort(hTable))
hdf=data.frame(hTable)
hdf=subset(hdf,nchar(as.character(hdf$Var1))>1& hdf$Freq>100)
hdf
wordcloud2(hdf,shape = "diamond")
words=lapply(X=res1,FUN=segmentCN)
class(words)
word=lapply(X=words,FUN=strsplit,' ')
words=unlist(lapply(X=res1,FUN=segmentCN))
hTable=table(words)
hdf=data.frame(hTable)
hdf=subset(hdf,nchar(as.character(hdf$Var1))>1& hdf$Freq>100)
hdf
wordcloud2(hdf,shape = "diamond")
View(words)
words=unlist(lapply(X=res1,FUN=segmentCN))
View(words)
class(words)
class(word)
hTable=table(words)
hdf=data.frame(hTable)
wordcloud2(hdf,shape = "diamond")
hdf=subset(hdf,nchar(as.character(hdf$Var1))>1& hdf$Freq>100)
hdf
wordcloud2(hdf,shape = "diamond")
hdf
hdf=data.frame(hTable)
hdf
hdf=subset(hdf,nchar(as.character(hdf$Var1))>1& hdf$Freq>100)
hdf
hdf=subset(hdf,nchar(as.character(hdf$words))>1& hdf$Freq>100)
hdf
hdf=data.frame(hTable)
hdf=subset(hdf,nchar(as.character(hdf$words))>1& hdf$Freq>100)
hdf
wordcloud2(hdf,shape = "diamond")
htest=segmentCN(file.choose(),returnType = tm)
class=htest
class(htest)
head(htest)
?segmentCN
??segmentCN
htest=segmentCN(file.choose(),returnType = tm)
class(htest)
head(htest)
class(words)
word=unlist(lapply(X=words,FUN=strsplit,' '))
hTable=table(words)
hdf=data.frame(hTable)
hdf=subset(hdf,nchar(as.character(hdf$words))>1& hdf$Freq>100)
hdf
res2=read.csv(file.choose(),fileEncoding ="UTF-8", stringsAsFactors = FALSE,header = FALSE)[]
hTable2=table(unlist(lapply(X=res2,FUN=segmentCN())))
words2=unlist(lapply(X=res2,FUN=segmentCN))
htable2=htable(words2)
htable2=table(words2)
htable2
hdf=data.frame(table(words2))
hdf
hdf=subset(hdf,nchar(as.character(hdf$words2))>1&hdf$Freq>100)
wordcloud2(hdf)
hdf2=data.frame(words2)
hdf2$words2
hdf2
?table
install.packages("devtools")
install_github("amplab-extras/SparkR-pkg", subdir="pkg")
install.packages("spark")
?spark
??spark
install.packages(c("git2r", "xml2", "rversions"))
librar(devtools)
library(devtools)
install_github("amplab-extras/SparkR-pkg", subdir="pkg")
install.packages("sparklyr")
library(sparklyr)
library(sparklyr)
spark_install(version = "1.6.1")
spark_install(version = "2.1.1")
spark_available_versions()
spark_install(version = "2.0.2")
install_github("amplab-extras/SparkR-pkg", subdir="pkg")
library(devtools)
install_github("amplab-extras/SparkR-pkg", subdir="pkg")
library(sparklyr)
library(dplyr)
sc=spark_connect(master="local")
sc= spark_connect(master = "spark://local:7077")
library(rJava)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_91')
library(rJava)
sc= spark_connect(master = "spark://local:7077")
Sys.setenv(SPARK_HOME='\\opt\\spark\\spark-2.0.2-bin-hadoop2.7')
sc= spark_connect(master = "spark://local:7077")
install.packages("C:/Users/yzm/Desktop/spark-2.1.1-bin-hadoop2.7/spark-2.1.1-bin-hadoop2.7/R/lib/sparkr.zip", repos = NULL, type = "win.binary")
library(sparkr)
library(SparkR)
?SparkR
??SparkR
library(SparkR)
sparkR.session(appName = "SparkR-DataFrame-example")
localDF <- data.frame(name=c("John", "Smith", "Sarah"), age=c(19, 23, 18))
df <- createDataFrame(localDF)
printSchema(df)
sparkR.session(appName = "SparkR-DataFrame-example")
localDF <- data.frame(name=c("John", "Smith", "Sarah"), age=c(19, 23, 18))
localDF
df <- createDataFrame(localDF)
sparkR.session(appName = "SparkR-DataFrame-example")
sc <- sparkR.init(master="local")
library(SparkR)
sc <- sparkR.init(master=" spark://localhost:7077",sparkEnvir=list(spark.executor.memory="1g",spark.cores.max="10"))
Sys.setenv(SPARK_HOME='C:\\Users\\yzm\\AppData\\Local\\rstudio\\spark\\Cache\\spark-2.0.2-bin-hadoop2.7')
sc <- sparkR.session(master=" spark://localhost:7077",sparkEnvir=list(spark.executor.memory="1g",spark.cores.max="10"))
sc <- sparkR.init(master="local")
sc <- sparkR.init(master="local")
sparkR.session(appName = "SparkR-DataFrame-example")
localDF <- data.frame(name=c("John", "Smith", "Sarah"), age=c(19, 23, 18))
localDF
df <- createDataFrame(localDF)
path <- file.path(Sys.getenv("SPARK_HOME"), "examples/src/main/resources/people.json")
peopleDF <- read.json(path)
sparkR.session(appName = "SparkR-DataFrame-example")
sc <- sparkR.session(master=" spark://localhost:7077")
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_91')
Sys.setenv(SPARK_HOME='C:\\Users\\yzm\\AppData\\Local\\rstudio\\spark\\Cache\\spark-2.0.2-bin-hadoop2.7')
library(sparklyr)
library(rJava)
library(dplyr)
library(SparkR)
sc <- sparkR.session(master=" spark://localhost:7077")
library(SparkR)
# Initialize SparkSession
sparkR.session(appName = "SparkR-DataFrame-example")
sc <- sparkR.session(master=" spark://localhost:4040")
spark_disconnect(sc)
hdf=data.frame(hTable)
wordcloud2(hdf,shape = "diamond")
library(wordcloud2)
wordcloud2(hdf)
hdf=subset(hdf,nchar(as.character(hdf$words2))>1&hdf$Freq>100)
wordcloud2(hdf)
hdf=subset(hdf,nchar(as.character(hdf$words2))>1&hdf$Freq>100)
wordcloud2(hdf)
hdf
hdf=data.frame(table(words2))
hdf
hdf=subset(hdf,nchar(as.character(hdf$words2))>1& hdf$Freq>100)
hdf
wordcloud2(hdf)
wordcloud2(hdf)
rPlot(mpg ~ wt , data = mtcars, color = 'wt', type = 'point')
install.packages("rvest")
library(rvest)
webpage=read_html(url)
url='http://www.diamondse.info/webService.php?shape=none&minCarat=0.2&maxCarat=30&minColor=1&maxColor=9&minPrice=100&maxPrice=1000000&minCut=5&maxCut=1&minClarity=1&maxClarity=10&minDepth=0&maxDepth=90&minWidth=0&maxWidth=90&gia=1&ags=1&egl=0&oth=0&currency=USD&rowStart=40&sortCol=price&sortDir=ASC'
webpage=read_html(url)
webpage
rank_data_html <- html_nodes(webpage,'td')
rank_data_html
head(rank_data_html,50)
head(rank_data_html,30)
rank_data_html
for(i in rank_data_html){
print(paste(" nbode",rank_data_html[i]))
}
rank_data_html[40]
rank_data_html[2]
rank_data_html[2]
rank_data_html[15]
rank_data_html[28]
rank_data_html[31]
rank_data_html[41]
install.packages("parallel")
install.packages("foreach")
install.packages("snow")
system.time({
library(MASS)
result <- kmeans(Boston,4,nstart=100000)
})
head(Boston)
library(snow)
source('C:/Users/yzm/Desktop/spider.r')
system.time({
data(Boston)
cl <- makeCluster(2,type='SOCK')
clusterExport(cl,'Boston')
results <- clusterApply(cl,rep(50000,2),function(nstart) kmean(Boston,4,nstart=nstart))
i <- sapply(results,function(result) result$tot.withinss)
result <- results[[which.min(i)]]
stopCluster(cl)
})
system.time({
data(Boston)
cl <- makeCluster(2,type='SOCK')
clusterExport(cl,'Boston')
results <- clusterApply(cl,rep(50000,2),function(nstart) kmean(Boston,4,nstart=nstart))
i <- sapply(results,function(result) result$tot.withinss)
result <- results[[which.min(i)]]
stopCluster(cl)
})
system.time({
data(Boston)
cl <- makeCluster(2,type='SOCK')
clusterExport(cl,'Boston')
results <- clusterApply(cl,rep(50000,2),function(nstart) kmeans(Boston,4,nstart=nstart))
i <- sapply(results,function(result) result$tot.withinss)
result <- results[[which.min(i)]]
stopCluster(cl)
})
system.time({
data(Boston)
cl <- makeCluster(2,type='SOCK')
clusterExport(cl,'Boston')
results <- clusterApply(cl,rep(50000,2),function(nstart) kmeans(Boston,4,nstart=nstart))
i <- sapply(results,function(result) result$tot.withinss)
result <- results[[which.min(i)]]
stopCluster(cl)
})
?makeCluster
install.packages("doParallel")
library(parallel)
library(foreach)
library(doParallel)
func <- function(x) {
n = 1
raw <- x
while (x > 1) {
x <- ifelse(x%%2==0,x/2,3*x+1)
n = n + 1
}
return(c(raw,n))
}
func()
func(100)
func(10000000)
func(1000000000000)
func(10000000000)
func(1000000000)
func(837799)
func(1e6)
system.time({
x <- 1:1e6
cl <- makeCluster(4)  # 初始化四核心集群
results <- parLapply(cl,x,func) # lapply的并行版本
res.df <- do.call('rbind',results) # 整合结果
stopCluster(cl) # 关闭集群
})
# 找到最大的步数对应的数字
res.df[which.max(res.df[,2]),1]
system.time({
x <- 1:1e6
cl <- makeCluster(8)  # 初始化四核心集群
results <- parLapply(cl,x,func) # lapply的并行版本
res.df <- do.call('rbind',results) # 整合结果
stopCluster(cl) # 关闭集群
})
# 找到最大的步数对应的数字
res.df[which.max(res.df[,2]),1]
x <- foreach(x=1:1000,.combine='rbind') %do% func(x)
x
cl <- makeCluster(4)
registerDoParallel(cl)
# 并行计算方式
x <- foreach(x=1:1000,.combine='rbind') %dopar% func(x)
stopCluster(cl)
x
system.time({
x <- foreach(x=1:1000,.combine='rbind') %do% func(x)
})
system.time({
cl <- makeCluster(4)
registerDoParallel(cl)
# 并行计算方式
x <- foreach(x=1:1000,.combine='rbind') %dopar% func(x)
stopCluster(cl)
})
cl <- makeCluster(4)
registerDoParallel(cl)
system.time({
# 并行计算方式
x <- foreach(x=1:1000,.combine='rbind') %dopar% func(x)
})
stopCluster(cl)
system.time({
x <- foreach(x=1:1e6,.combine='rbind') %do% func(x)
})
system.time({
x <- foreach(x=1:1e3,.combine='rbind') %do% func(x)
})
system.time({
x <- foreach(x=1:1e3,.combine='rbind') %do% func(x)
})
c
c
c
c
c
c
c
c
c
Q
Q
Q
system.time({
x <- foreach(x=1:1e3,.combine='rbind') %do% func(x)
})
cl <- makeCluster(8)
registerDoParallel(cl)
system.time({
# 并行计算方式
x <- foreach(x=1:1e3,.combine='rbind') %dopar% func(x)
})
stopCluster(cl)
system.time({
x <- foreach(x=1:1e4,.combine='rbind') %do% func(x)
})
cl <- makeCluster(8)
registerDoParallel(cl)
system.time({
# 并行计算方式
x <- foreach(x=1:1e4,.combine='rbind') %dopar% func(x)
})
stopCluster(cl)
system.time({
x <- foreach(x=1:1e4,.combine='rbind') %do% func(x)
})
system.time({
# 并行计算方式
x <- foreach(x=1:1e4,.combine='rbind') %dopar% func(x)
})
cl <- makeCluster(8)
registerDoParallel(cl)
system.time({
# 并行计算方式
x <- foreach(x=1:1e4,.combine='rbind') %dopar% func(x)
})
stopCluster(cl)
cl <- makeCluster(4)
registerDoParallel(cl)
system.time({
# 并行计算方式
x <- foreach(x=1:1e4,.combine='rbind') %dopar% func(x)
})
stopCluster(cl)
cl <- makeCluster(2)
registerDoParallel(cl)
system.time({
# 并行计算方式
x <- foreach(x=1:1e4,.combine='rbind') %dopar% func(x)
})
stopCluster(cl)
system.time({
x <- foreach(x=1:2e4,.combine='rbind') %do% func(x)
})
cl <- makeCluster(4)
registerDoParallel(cl)
system.time({
# 并行计算方式
x <- foreach(x=1:2e4,.combine='rbind') %dopar% func(x)
})
stopCluster(cl)
cl <- makeCluster(8)
registerDoParallel(cl)
system.time({
# 并行计算方式
x <- foreach(x=1:2e4,.combine='rbind') %dopar% func(x)
})
stopCluster(cl)
cl <- makeCluster(8)
registerDoParallel(cl)
rf <- foreach(ntree=rep(25000, 4),
.combine=combine,
.packages='randomForest') %dopar%
randomForest(Species~., data=iris, ntree=ntree)
stopCluster(cl)
?foreach
cl <- makeCluster(8)
registerDoParallel(cl)
rf <- foreach(ntree=rep(25000, 4),
.combine=c,
.packages='randomForest') %dopar%
randomForest(Species~., data=iris, ntree=ntree)
stopCluster(cl)
rf
draw(rf)
print(rf)
plot(rf)
cl.cores <- detectCores()
cl.cores
